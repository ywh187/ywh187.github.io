<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Weihao Ye</title>

  <meta name="author" content="Weihao Ye">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/sunflower.svg" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Weihao Ye (叶伟豪)</name>
              </p>
              <p>
                I am a second-year Master's student at <a href="https://www.xmu.edu.cn/">Xiamen University</a>.
                My supervisors are <a href="https://mac.xmu.edu.cn/rrji/">Prof. Rongrong Ji</a> and <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=en">Prof. Yiyi Zhou</a>.
                I obtained an Honors Bachelor's degree in Computer Science from <a href="https://en.szu.edu.cn">SZU</a> in 2023, supervised by <a href="https://scholar.google.com/citations?user=XNDHhIEAAAAJ&hl=zh-CN">Prof. Xu Wang</a>.
              </p>

              <!-- <heading>Research</heading> -->
              <p>
                My research interest lies in <strong>efficiency optimization</strong> for multimodal understanding and generation models. 
                I aim to develop techniques that significantly reduce the <strong>resource requirements</strong> of multimodal models, making them more suitable for <strong>real-world deployment</strong>. 
                Looking ahead, I am also interested in exploring <strong>reinforcement learning</strong> to guide and refine model behavior in complex, interactive environments.
              </p>

              <p style="text-align:center">
                <a href="mailto:ywh187@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://github.com/ywh187">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=JrVxIV0AAAAJ&hl=zh-CN">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/weihao.jpg">
                <img style="width:100%;max-width:100%" alt="profile photo" src="images/weihao.jpg" class="hoverZoomLink">
              </a>
            </td>
          </tr>
        </tbody></table>

        <!-- Long-Term Research Goal Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Long-Term Research Goal</heading>
                <p>
                  My long-term research goal is to develop systems <strong>from the bottom up</strong>, 
                  aligning <strong>low-level algorithmic efficiency</strong> with <strong>the practical needs of real-world applications</strong>.
                </p>
                <p style="text-align:center">
                  <img src="images/LongTerm.png" alt="Long-Term Goal Illustration" style="width:90%;max-width:700px;margin-top:20px;">
                </p>
              </td>
            </tr>
          </tbody>
        </table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        
        <p style="margin-left: 20px;">Check my <a href="https://scholar.google.com/citations?user=JrVxIV0AAAAJ&hl=zh-CN">Google Scholar</a> for the most updated list of publications.</p>
       
        <!-- Card 1 -->
        <div style="display:flex;align-items:center;border:1px solid #ddd;border-radius:10px;padding:15px;margin:20px auto;width:95%;box-shadow:0 2px 5px rgba(0,0,0,0.1);">
          <img src="images/fitprune.png" alt="Paper Thumbnail" style="width:140px;height:auto;border-radius:8px;margin-right:20px;">
          <div>
            <a href="https://arxiv.org/pdf/2409.10197v2" style="font-weight:bold;font-size:16px;text-decoration:none;color:#1a0dab;">[1] Fit and Prune: Fast and Training-Free Visual Token Pruning for Multi-modal Large Language Models</a>
            <p><strong>Weihao Ye</strong>, Qiong Wu, Wenhao Lin, Yiyi Zhou. <em>AAAI 2025</em> (Citations: 15)</p>
            <p>This paper proposes a training-free visual token pruning method based on attention distribution fitting, significantly boosting inference efficiency of MLLMs.</p>
          </div>
        </div>

        <!-- Card 2 -->
        <div style="display:flex;align-items:center;border:1px solid #ddd;border-radius:10px;padding:15px;margin:20px auto;width:95%;box-shadow:0 2px 5px rgba(0,0,0,0.1);">
          <img src="images/dyvte.png" alt="Paper Thumbnail" style="width:140px;height:auto;border-radius:8px;margin-right:20px;">
          <div>
            <a href="https://arxiv.org/pdf/2411.19628" style="font-weight:bold;font-size:16px;text-decoration:none;color:#1a0dab;">[2] Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings</a>
            <p>Qiong Wu, Wenhao Lin, <strong>Weihao Ye</strong>, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji. <em>NeurIPS 2025 (under review)</em> (Citations: 3)</p>
            <p>This work introduces a dynamic visual token exit mechanism to accelerate MLLMs, along with extensive empirical studies.</p>
          </div>
        </div>

        <!-- Card 3 -->
        <div style="display:flex;align-items:center;border:1px solid #ddd;border-radius:10px;padding:15px;margin:20px auto;width:95%;box-shadow:0 2px 5px rgba(0,0,0,0.1);">
          <img src="images/skipattn.png" alt="Paper Thumbnail" style="width:140px;height:auto;border-radius:8px;margin-right:20px;">
          <div>
            <a href="https://arxiv.org/pdf/2403.15226" style="font-weight:bold;font-size:16px;text-decoration:none;color:#1a0dab;">[3] Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models</a>
            <p>Qiong Wu, <strong>Weihao Ye</strong>, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji. <em>IJCV (under review)</em> (Citations: 8)</p>
            <p>This paper proposes an efficient fine-tuning method that skips redundant attention heads using Propagation-Information Adapter (PIA), reducing computational cost.</p>
          </div>
        </div>

        <!-- Card 4 -->
        <div style="display:flex;align-items:center;border:1px solid #ddd;border-radius:10px;padding:15px;margin:20px auto;width:95%;box-shadow:0 2px 5px rgba(0,0,0,0.1);">
          <img src="images/panorama.png" alt="Paper Thumbnail" style="width:140px;height:auto;border-radius:8px;margin-right:20px;">
          <div>
            <a href="https://ieeexplore.ieee.org/document/10032431" style="font-weight:bold;font-size:16px;text-decoration:none;color:#1a0dab;">[4] A Learning-based Framework for Multi-View Instance Segmentation in Panorama</a>
            <p><strong>Weihao Ye*</strong>, Ziyang Mai*, Qiudan Zhang, Xu Wang. <em>DSAA 2022 (CCF-C)</em> (Citations: 1)</p>
            <p>This work introduces a multi-view joint framework for panoramic instance segmentation, improving segmentation accuracy.</p>
          </div>
        </div>
        <!--  -->


        <!-- Internship Section -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Internship</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:20%;vertical-align:middle;text-align:center;">
                <img src="images/bytedance.png" alt="ByteDance Logo" style="width:80%;max-width:150px;">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle;">
                <p>
                  I worked as a research intern at <strong>ByteDance</strong>, where I contributed to the acceleration and optimization of 
                  <strong>image and video generation models</strong> used in products such as <strong>Doubao</strong> and <strong>Dreamina</strong>.
                </p>
              </td>
            </tr>
          </tbody>
        </table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <div >
                <li>[2024] Selected for Tsinghua & Tencent SDG Project Grant Program, responsible for software development. <a href="https://techforgood.qq.com/projects/detail/324" target="_blank">[link]</a></li>
                <li>[2023] SZU Outstanding Graduate, SZU</li>
                <li>[2023] Third Prize (GBA Region), Huawei Software Elite Challenge – Multi-Robot Collaboration, responsible for motion and collision algorithms.</li>
                <li>[2022] Gold Award of Game for Peace Developer Contest, Tencent, team leader & game programmer. <a href="https://gp.qq.com/gicp/news/684/18077950.html" target="_blank">[link]</a></li>
                <li>[2022] National Third Prize of Lanqiao Programming Competition.</li>
                <li>[2022] Provincial Second Prize of National College Digital Art & Design Competition, team leader & main programmer.</li>
                <li>[2020–2022] university-level honors during undergraduate studies, including Star of Academics and Star of Innovation & Entrepreneurship.</li>
              </div>
            </td>
            </tr>
          </tbody>
        </table>

      </td>
    </tr>
  </table>
</body>

</html>